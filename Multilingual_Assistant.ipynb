{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxFpYjR9TU3M"
      },
      "outputs": [],
      "source": [
        "!pip install groq gradio langdetect gtts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from groq import Groq\n",
        "from langdetect import detect\n",
        "from gtts import gTTS\n",
        "import base64"
      ],
      "metadata": {
        "id": "wI6pyJngTiDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY = input(\"Enter your Groq API key: \")\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])"
      ],
      "metadata": {
        "id": "n3KtI73kTkeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_input(text):\n",
        "    if not text:\n",
        "        return \"Please provide text input.\", None\n",
        "    lang = detect(text)\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": text}],\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            temperature=0.7,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        response = chat_completion.choices[0].message.content\n",
        "        return response, lang\n",
        "    except Exception as e:\n",
        "        return f\"Error processing text: {str(e)}\", None"
      ],
      "metadata": {
        "id": "gp7Jj2pkTuDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio_input(audio):\n",
        "    if not audio:\n",
        "        return \"Please provide audio input.\", None\n",
        "    try:\n",
        "        with open(audio, \"rb\") as file:\n",
        "            transcription = client.audio.transcriptions.create(\n",
        "                file=(audio, file.read()),\n",
        "                model=\"whisper-large-v3\",\n",
        "                response_format=\"text\"\n",
        "            )\n",
        "        lang = detect(transcription)\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": transcription}],\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            temperature=0.7,\n",
        "            max_tokens=1024\n",
        "        ).choices[0].message.content\n",
        "        return response, lang\n",
        "    except Exception as e:\n",
        "        return f\"Error processing audio: {str(e)}\", None"
      ],
      "metadata": {
        "id": "Bq_XfLxMTwD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image_input(image, text_prompt):\n",
        "    if not image or not text_prompt:\n",
        "        return \"Please provide an image and a text prompt.\", None\n",
        "    try:\n",
        "        with open(image, \"rb\") as img_file:\n",
        "            image_data = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
        "\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": text_prompt},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}}\n",
        "                ]\n",
        "            }],\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            temperature=0.7,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        response = chat_completion.choices[0].message.content\n",
        "        lang = detect(text_prompt)\n",
        "        return response, lang\n",
        "    except Exception as e:\n",
        "        return f\"Error processing image: {str(e)}\", None"
      ],
      "metadata": {
        "id": "loK10SJ6Txru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_speech(text, lang):\n",
        "    try:\n",
        "        supported_langs = ['en', 'es', 'fr', 'de', 'it', 'zh-cn', 'ja', 'ko']\n",
        "        if lang not in supported_langs:\n",
        "            lang = 'en'\n",
        "        tts = gTTS(text=text, lang=lang, slow=False)\n",
        "        audio_file = \"response.mp3\"\n",
        "        tts.save(audio_file)\n",
        "        return audio_file\n",
        "    except Exception as e:\n",
        "        return None"
      ],
      "metadata": {
        "id": "d9_uBWjHTzJf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assistant(text_input, audio_input, image_input, image_prompt):\n",
        "    response_text = \"\"\n",
        "    audio_output = None\n",
        "\n",
        "    if text_input:\n",
        "        response_text, lang = process_text_input(text_input)\n",
        "    elif audio_input:\n",
        "        response_text, lang = process_audio_input(audio_input)\n",
        "    elif image_input and image_prompt:\n",
        "        response_text, lang = process_image_input(image_input, image_prompt)\n",
        "    else:\n",
        "        response_text = \"Please provide at least one input (text, audio, or image with prompt).\"\n",
        "        lang = None\n",
        "\n",
        "    if response_text and lang:\n",
        "        audio_output = text_to_speech(response_text, lang)\n",
        "\n",
        "    return response_text, audio_output"
      ],
      "metadata": {
        "id": "AtZagHp5Tnhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface = gr.Interface(\n",
        "    fn=assistant,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Type your message (e.g., 'Hola, ¿qué tal?')\"),\n",
        "        gr.Audio(label=\"Record or upload audio\", type=\"filepath\"),\n",
        "        gr.Image(label=\"Upload an image\", type=\"filepath\"),\n",
        "        gr.Textbox(label=\"Image prompt (e.g., 'What is this?')\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Response\"),\n",
        "        gr.Audio(label=\"Listen to Response\", type=\"filepath\")\n",
        "    ],\n",
        "    title=\"Multilingual AI Assistant Powered by Groq\",\n",
        "    description=\"Input text, audio, or an image with a prompt. Get responses in text and audio!\",\n",
        "    live=False\n",
        ")"
      ],
      "metadata": {
        "id": "5m830rTRT8EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "-Sq9itsEUUI2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}